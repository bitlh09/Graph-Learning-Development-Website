import numpy as np
import os
from collections import defaultdict
import time

print("GraphSAGEå®Œæ•´è®­ç»ƒå®ç°")
print("=" * 50)

class GraphSAGETrainer:
    """GraphSAGEæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01):
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.learning_rate = learning_rate
        
        # æƒé‡åˆå§‹åŒ– (Xavieråˆå§‹åŒ–)
        self.W1 = np.random.randn(input_dim * 2, hidden_dim) * np.sqrt(2.0 / (input_dim * 2))
        self.b1 = np.zeros(hidden_dim)
        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)
        self.b2 = np.zeros(output_dim)
        
        # ç”¨äºå­˜å‚¨å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ
        self.cache = {}
        
    def relu(self, x):
        return np.maximum(0, x)
    
    def relu_backward(self, dout, x):
        """ReLUçš„åå‘ä¼ æ’­"""
        dx = dout.copy()
        dx[x <= 0] = 0
        return dx
    
    def softmax(self, x):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def cross_entropy_loss(self, y_pred, y_true):
        """äº¤å‰ç†µæŸå¤±"""
        m = y_pred.shape[0]
        y_pred_clipped = np.clip(y_pred, 1e-12, 1 - 1e-12)
        
        # åˆ›å»ºone-hotç¼–ç 
        y_true_onehot = np.zeros_like(y_pred)
        y_true_onehot[np.arange(m), y_true] = 1
        
        # è®¡ç®—æŸå¤±
        loss = -np.sum(y_true_onehot * np.log(y_pred_clipped)) / m
        return loss
    
    def aggregate_neighbors(self, features, adj_list, nodes):
        """èšåˆé‚»å±…ç‰¹å¾"""
        agg_features = []
        for node in nodes:
            neighbors = adj_list.get(node, [])
            if len(neighbors) == 0:
                agg_features.append(np.zeros(features.shape[1]))
            else:
                valid_neighbors = [n for n in neighbors if n < len(features)]
                if len(valid_neighbors) == 0:
                    agg_features.append(np.zeros(features.shape[1]))
                else:
                    neighbor_feat = features[valid_neighbors]
                    agg_features.append(np.mean(neighbor_feat, axis=0))
        return np.array(agg_features)
    
    def forward(self, features, adj_list, nodes):
        """å‰å‘ä¼ æ’­"""
        batch_size = len(nodes)
        
        # èšåˆé‚»å±…ç‰¹å¾
        neighbor_feat = self.aggregate_neighbors(features, adj_list, nodes)
        self_feat = features[nodes]
        
        # è¿æ¥è‡ªèº«å’Œé‚»å±…ç‰¹å¾
        combined = np.concatenate([self_feat, neighbor_feat], axis=1)
        
        # ç¬¬ä¸€å±‚
        z1 = np.dot(combined, self.W1) + self.b1
        h1 = self.relu(z1)
        
        # è¾“å‡ºå±‚
        z2 = np.dot(h1, self.W2) + self.b2
        y_pred = self.softmax(z2)
        
        # ä¿å­˜ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­
        self.cache = {
            'combined': combined,
            'z1': z1,
            'h1': h1,
            'z2': z2,
            'y_pred': y_pred
        }
        
        return y_pred
    
    def backward(self, y_true):
        """åå‘ä¼ æ’­"""
        # è·å–ç¼“å­˜çš„å€¼
        combined = self.cache['combined']
        z1 = self.cache['z1']
        h1 = self.cache['h1']
        z2 = self.cache['z2']
        y_pred = self.cache['y_pred']
        
        batch_size = y_pred.shape[0]
        
        # åˆ›å»ºone-hotç¼–ç çš„çœŸå®æ ‡ç­¾
        y_true_onehot = np.zeros_like(y_pred)
        y_true_onehot[np.arange(batch_size), y_true] = 1
        
        # è¾“å‡ºå±‚çš„æ¢¯åº¦
        dz2 = (y_pred - y_true_onehot) / batch_size
        
        # W2å’Œb2çš„æ¢¯åº¦
        dW2 = np.dot(h1.T, dz2)
        db2 = np.sum(dz2, axis=0)
        
        # éšè—å±‚çš„æ¢¯åº¦
        dh1 = np.dot(dz2, self.W2.T)
        dz1 = self.relu_backward(dh1, z1)
        
        # W1å’Œb1çš„æ¢¯åº¦
        dW1 = np.dot(combined.T, dz1)
        db1 = np.sum(dz1, axis=0)
        
        # æ›´æ–°æƒé‡
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
    
    def predict(self, features, adj_list, nodes):
        """é¢„æµ‹"""
        y_pred = self.forward(features, adj_list, nodes)
        return np.argmax(y_pred, axis=1)
    
    def train_step(self, features, adj_list, nodes, labels):
        """å•æ­¥è®­ç»ƒ"""
        # å‰å‘ä¼ æ’­
        y_pred = self.forward(features, adj_list, nodes)
        
        # è®¡ç®—æŸå¤±
        loss = self.cross_entropy_loss(y_pred, labels)
        
        # åå‘ä¼ æ’­
        self.backward(labels)
        
        # è®¡ç®—å‡†ç¡®ç‡
        predictions = np.argmax(y_pred, axis=1)
        accuracy = np.mean(predictions == labels)
        
        return loss, accuracy


def load_citeseer_data():
    """åŠ è½½citeseeræ•°æ®"""
    print("åŠ è½½Citeseeræ•°æ®é›†...")
    
    content_file = "data/citeseer/citeseer.content"
    cites_file = "data/citeseer/citeseer.cites"
    
    # è¯»å–èŠ‚ç‚¹ç‰¹å¾å’Œæ ‡ç­¾
    node_id_to_idx = {}
    features_list = []
    labels_list = []
    
    with open(content_file, 'r') as f:
        for idx, line in enumerate(f):
            parts = line.strip().split('\t')
            node_id = parts[0]
            features = [float(x) for x in parts[1:-1]]
            label = parts[-1]
            
            node_id_to_idx[node_id] = idx
            features_list.append(features)
            labels_list.append(label)
    
    # åˆ›å»ºæ ‡ç­¾æ˜ å°„
    unique_labels = list(set(labels_list))
    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
    numeric_labels = [label_to_idx[label] for label in labels_list]
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    features = np.array(features_list, dtype=np.float32)
    labels = np.array(numeric_labels, dtype=np.int32)
    
    # ç‰¹å¾å½’ä¸€åŒ–
    features = (features - np.mean(features, axis=0)) / (np.std(features, axis=0) + 1e-8)
    
    # è¯»å–è¾¹ä¿¡æ¯
    adj_list = defaultdict(list)
    edge_count = 0
    
    with open(cites_file, 'r') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) == 2:
                cited_id, citing_id = parts
                if cited_id in node_id_to_idx and citing_id in node_id_to_idx:
                    cited_idx = node_id_to_idx[cited_id]
                    citing_idx = node_id_to_idx[citing_id]
                    adj_list[cited_idx].append(citing_idx)
                    adj_list[citing_idx].append(cited_idx)
                    edge_count += 1
    
    # å»é‡
    for node in adj_list:
        adj_list[node] = list(set(adj_list[node]))
    
    print(f"æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  èŠ‚ç‚¹æ•°: {len(features)}")
    print(f"  ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    print(f"  ç±»åˆ«æ•°: {len(unique_labels)}")
    print(f"  è¾¹æ•°: {edge_count}")
    print(f"  ç±»åˆ«: {unique_labels}")
    
    return features, labels, adj_list, label_to_idx, unique_labels


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    
    # åŠ è½½æ•°æ®
    features, labels, adj_list, label_to_idx, unique_labels = load_citeseer_data()
    
    # æ•°æ®åˆ†å‰²
    num_nodes = len(features)
    indices = np.random.permutation(num_nodes)
    
    train_size = int(0.6 * num_nodes)
    val_size = int(0.2 * num_nodes)
    
    train_indices = indices[:train_size]
    val_indices = indices[train_size:train_size + val_size]
    test_indices = indices[train_size + val_size:]
    
    print(f"\næ•°æ®åˆ†å‰²:")
    print(f"  è®­ç»ƒé›†: {len(train_indices)}")
    print(f"  éªŒè¯é›†: {len(val_indices)}")
    print(f"  æµ‹è¯•é›†: {len(test_indices)}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    input_dim = features.shape[1]
    hidden_dim = 128
    output_dim = len(label_to_idx)
    learning_rate = 0.01
    
    model = GraphSAGETrainer(input_dim, hidden_dim, output_dim, learning_rate)
    
    print(f"\næ¨¡å‹å‚æ•°:")
    print(f"  è¾“å…¥ç»´åº¦: {input_dim}")
    print(f"  éšè—ç»´åº¦: {hidden_dim}")
    print(f"  è¾“å‡ºç»´åº¦: {output_dim}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    
    # è®­ç»ƒå‚æ•°
    epochs = 50
    batch_size = 32
    
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    print(f"  è½®æ•°: {epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print("-" * 50)
    
    # è®­ç»ƒè®°å½•
    train_losses = []
    train_accuracies = []
    val_accuracies = []
    
    start_time = time.time()
    
    for epoch in range(epochs):
        # æ‰“ä¹±è®­ç»ƒæ•°æ®
        np.random.shuffle(train_indices)
        
        epoch_losses = []
        epoch_accuracies = []
        
        # æ‰¹æ¬¡è®­ç»ƒ
        for i in range(0, len(train_indices), batch_size):
            batch_indices = train_indices[i:i+batch_size]
            batch_labels = labels[batch_indices]
            
            # è®­ç»ƒæ­¥éª¤
            loss, accuracy = model.train_step(features, adj_list, batch_indices, batch_labels)
            
            epoch_losses.append(loss)
            epoch_accuracies.append(accuracy)
        
        # è®¡ç®—éªŒè¯å‡†ç¡®ç‡
        val_predictions = model.predict(features, adj_list, val_indices)
        val_accuracy = np.mean(val_predictions == labels[val_indices])
        
        # è®°å½•æŒ‡æ ‡
        avg_loss = np.mean(epoch_losses)
        avg_train_accuracy = np.mean(epoch_accuracies)
        
        train_losses.append(avg_loss)
        train_accuracies.append(avg_train_accuracy)
        val_accuracies.append(val_accuracy)
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 5 == 0 or epoch == 0:
            elapsed_time = time.time() - start_time
            print(f"Epoch {epoch+1:2d}/{epochs}: Loss: {avg_loss:.4f}, "
                  f"Train Acc: {avg_train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, "
                  f"Time: {elapsed_time:.1f}s")
    
    print("-" * 50)
    print("è®­ç»ƒå®Œæˆï¼")
    
    # æµ‹è¯•é›†è¯„ä¼°
    print("\næœ€ç»ˆè¯„ä¼°:")
    test_predictions = model.predict(features, adj_list, test_indices)
    test_accuracy = np.mean(test_predictions == labels[test_indices])
    
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
    
    # å„ç±»åˆ«å‡†ç¡®ç‡
    print("\nå„ç±»åˆ«æµ‹è¯•å‡†ç¡®ç‡:")
    for class_idx, class_name in enumerate(unique_labels):
        class_mask = labels[test_indices] == class_idx
        if np.sum(class_mask) > 0:
            class_predictions = test_predictions[class_mask]
            class_labels = labels[test_indices][class_mask]
            class_accuracy = np.mean(class_predictions == class_labels)
            print(f"  {class_name}: {class_accuracy:.4f} ({np.sum(class_mask)} samples)")
    
    # æ˜¾ç¤ºè®­ç»ƒæ›²çº¿æ•°æ®
    print("\nè®­ç»ƒæ›²çº¿æ•°æ®:")
    print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {train_accuracies[-1]:.4f}")
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_accuracies[-1]:.4f}")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(val_accuracies):.4f} (Epoch {np.argmax(val_accuracies)+1})")
    
    total_time = time.time() - start_time
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’")
    
    print("\n" + "=" * 50)
    print("GraphSAGEè®­ç»ƒå®Œæˆï¼")
    print("å®ç°äº†å®Œæ•´çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œæ¢¯åº¦ä¸‹é™")
    print("==" * 25)
    
    return model, test_accuracy, val_accuracies


if __name__ == "__main__":
    model, test_acc, val_accs = main()
    print(f"\nğŸ‰ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")