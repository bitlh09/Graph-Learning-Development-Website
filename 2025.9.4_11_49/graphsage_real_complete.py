#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„ GraphSAGE å®ç°ï¼Œæ”¯æŒçœŸå® Citeseer æ•°æ®é›†
åŒ…å«å®Œæ•´çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œæ¢¯åº¦ä¸‹é™
"""

import numpy as np
import os
import random
from collections import defaultdict
import matplotlib.pyplot as plt
import time

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
random.seed(42)

class GraphSAGEComplete:
    """å®Œæ•´çš„ GraphSAGE å®ç°ï¼ŒåŒ…å«åå‘ä¼ æ’­"""
    
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, aggregator='mean'):
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.aggregator = aggregator
        
        # æƒé‡åˆå§‹åŒ–
        self.weights = []
        self.biases = []
        
        # ç¬¬ä¸€å±‚
        self.weights.append(self._xavier_init(input_dim * 2, hidden_dim))
        self.biases.append(np.zeros(hidden_dim))
        
        # ä¸­é—´å±‚
        for i in range(num_layers - 2):
            self.weights.append(self._xavier_init(hidden_dim * 2, hidden_dim))
            self.biases.append(np.zeros(hidden_dim))
        
        # è¾“å‡ºå±‚
        if num_layers > 1:
            self.weights.append(self._xavier_init(hidden_dim * 2, output_dim))
        else:
            self.weights.append(self._xavier_init(input_dim * 2, output_dim))
        self.biases.append(np.zeros(output_dim))
        
        # ç”¨äºå­˜å‚¨å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ
        self.cache = {}
    
    def _xavier_init(self, fan_in, fan_out):
        """Xavieråˆå§‹åŒ–"""
        limit = np.sqrt(6.0 / (fan_in + fan_out))
        return np.random.uniform(-limit, limit, (fan_in, fan_out))
    
    def _relu(self, x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)
    
    def _relu_derivative(self, x):
        """ReLUçš„å¯¼æ•°"""
        return (x > 0).astype(float)
    
    def _softmax(self, x):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def _aggregate_neighbors(self, node_features, adjacency_list, nodes):
        """èšåˆé‚»å±…èŠ‚ç‚¹ç‰¹å¾"""
        aggregated_features = []
        
        for node in nodes:
            neighbors = adjacency_list.get(node, [])
            if len(neighbors) == 0:
                aggregated_features.append(np.zeros(node_features.shape[1]))
            else:
                neighbor_features = node_features[neighbors]
                if self.aggregator == 'mean':
                    agg_feature = np.mean(neighbor_features, axis=0)
                elif self.aggregator == 'max':
                    agg_feature = np.max(neighbor_features, axis=0)
                else:
                    agg_feature = np.mean(neighbor_features, axis=0)
                aggregated_features.append(agg_feature)
        
        return np.array(aggregated_features)
    
    def forward(self, node_features, adjacency_list, target_nodes):
        """å‰å‘ä¼ æ’­"""
        self.cache = {'activations': [], 'z_values': [], 'combined_features': []}
        
        current_features = node_features.copy()
        self.cache['activations'].append(current_features)
        
        for layer in range(self.num_layers):
            # èšåˆé‚»å±…ç‰¹å¾
            neighbor_features = self._aggregate_neighbors(
                current_features, adjacency_list, target_nodes
            )
            
            # è·å–ç›®æ ‡èŠ‚ç‚¹çš„è‡ªèº«ç‰¹å¾
            self_features = current_features[target_nodes]
            
            # è¿æ¥è‡ªèº«ç‰¹å¾å’Œèšåˆçš„é‚»å±…ç‰¹å¾
            combined_features = np.concatenate([self_features, neighbor_features], axis=1)
            self.cache['combined_features'].append(combined_features)
            
            # çº¿æ€§å˜æ¢
            z = np.dot(combined_features, self.weights[layer]) + self.biases[layer]
            self.cache['z_values'].append(z)
            
            # æ¿€æ´»å‡½æ•°
            if layer < self.num_layers - 1:
                current_features = self._relu(z)
            else:
                current_features = z  # æœ€åä¸€å±‚ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
            
            self.cache['activations'].append(current_features)
        
        return current_features
    
    def backward(self, predictions, true_labels, learning_rate=0.01):
        """åå‘ä¼ æ’­"""
        batch_size = len(true_labels)
        
        # è®¡ç®—è¾“å‡ºå±‚çš„æ¢¯åº¦
        predictions_softmax = self._softmax(predictions)
        
        # åˆ›å»ºone-hotç¼–ç çš„çœŸå®æ ‡ç­¾
        y_true_onehot = np.zeros_like(predictions_softmax)
        y_true_onehot[np.arange(batch_size), true_labels] = 1
        
        # è¾“å‡ºå±‚æ¢¯åº¦
        delta = predictions_softmax - y_true_onehot
        
        # åå‘ä¼ æ’­æ›´æ–°æƒé‡
        for layer in reversed(range(self.num_layers)):
            # è·å–å½“å‰å±‚çš„è¾“å…¥
            layer_input = self.cache['combined_features'][layer]
            
            # è®¡ç®—æƒé‡æ¢¯åº¦
            weight_grad = np.dot(layer_input.T, delta) / batch_size
            bias_grad = np.mean(delta, axis=0)
            
            # æ›´æ–°æƒé‡å’Œåç½®
            self.weights[layer] -= learning_rate * weight_grad
            self.biases[layer] -= learning_rate * bias_grad
            
            # è®¡ç®—ä¸‹ä¸€å±‚çš„æ¢¯åº¦ï¼ˆå¦‚æœä¸æ˜¯ç¬¬ä¸€å±‚ï¼‰
            if layer > 0:
                # è®¡ç®—ä¼ é€’ç»™å‰ä¸€å±‚çš„æ¢¯åº¦
                delta_next = np.dot(delta, self.weights[layer].T)
                
                # åº”ç”¨æ¿€æ´»å‡½æ•°çš„å¯¼æ•°
                z_prev = self.cache['z_values'][layer - 1]
                delta = delta_next * self._relu_derivative(z_prev)
    
    def predict(self, node_features, adjacency_list, target_nodes):
        """é¢„æµ‹"""
        logits = self.forward(node_features, adjacency_list, target_nodes)
        probabilities = self._softmax(logits)
        return np.argmax(probabilities, axis=1)
    
    def compute_loss(self, predictions, true_labels):
        """è®¡ç®—äº¤å‰ç†µæŸå¤±"""
        predictions_softmax = self._softmax(predictions)
        batch_size = predictions.shape[0]
        log_probs = -np.log(predictions_softmax[np.arange(batch_size), true_labels] + 1e-8)
        return np.mean(log_probs)


def load_citeseer_data(data_path="data/citeseer", use_subset=False, subset_size=1000):
    """åŠ è½½ Citeseer æ•°æ®é›†"""
    print("=" * 60)
    print("ğŸ“š æ­£åœ¨åŠ è½½çœŸå® Citeseer æ•°æ®é›†...")
    print("=" * 60)
    
    content_file = os.path.join(data_path, "citeseer.content")
    cites_file = os.path.join(data_path, "citeseer.cites")
    
    if not os.path.exists(content_file) or not os.path.exists(cites_file):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    # å­˜å‚¨èŠ‚ç‚¹ä¿¡æ¯
    node_ids = []
    node_features_list = []
    node_labels_list = []
    
    print("ğŸ”„ è¯»å–èŠ‚ç‚¹ç‰¹å¾å’Œæ ‡ç­¾...")
    
    # è¯»å–èŠ‚ç‚¹å†…å®¹
    with open(content_file, 'r') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) < 3:
                continue
            
            node_id = parts[0]
            features = [float(x) for x in parts[1:-1]]
            label = parts[-1]
            
            node_ids.append(node_id)
            node_features_list.append(features)
            node_labels_list.append(label)
    
    print(f"âœ… åŸå§‹æ•°æ®é›†å¤§å°: {len(node_ids)} ä¸ªèŠ‚ç‚¹")
    
    # å¦‚æœä½¿ç”¨å­é›†ï¼Œè¿›è¡Œé‡‡æ ·
    if use_subset and len(node_ids) > subset_size:
        print(f"ğŸ¯ é‡‡æ · {subset_size} ä¸ªèŠ‚ç‚¹ä½œä¸ºè®­ç»ƒå­é›†...")
        
        # åˆ†å±‚é‡‡æ ·ç¡®ä¿æ¯ä¸ªç±»åˆ«éƒ½æœ‰ä»£è¡¨
        indices_by_label = defaultdict(list)
        for i, label in enumerate(node_labels_list):
            indices_by_label[label].append(i)
        
        selected_indices = []
        samples_per_class = subset_size // len(indices_by_label)
        
        for label, indices in indices_by_label.items():
            if len(indices) < samples_per_class:
                selected_indices.extend(indices)
            else:
                selected_indices.extend(random.sample(indices, samples_per_class))
        
        # å¦‚æœè¿˜éœ€è¦æ›´å¤šæ ·æœ¬ï¼Œéšæœºæ·»åŠ 
        if len(selected_indices) < subset_size:
            remaining = subset_size - len(selected_indices)
            all_indices = set(range(len(node_ids)))
            available = list(all_indices - set(selected_indices))
            selected_indices.extend(random.sample(available, min(remaining, len(available))))
        
        # é‡æ–°æ˜ å°„
        selected_indices = sorted(selected_indices[:subset_size])
        node_ids = [node_ids[i] for i in selected_indices]
        node_features_list = [node_features_list[i] for i in selected_indices]
        node_labels_list = [node_labels_list[i] for i in selected_indices]
        
        print(f"âœ… å­é›†é‡‡æ ·å®Œæˆ: {len(node_ids)} ä¸ªèŠ‚ç‚¹")
    
    # åˆ›å»ºèŠ‚ç‚¹IDåˆ°ç´¢å¼•çš„æ˜ å°„
    node_id_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}
    
    # åˆ›å»ºæ ‡ç­¾æ˜ å°„
    unique_labels = sorted(list(set(node_labels_list)))
    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
    idx_to_label = {idx: label for label, idx in label_to_idx.items()}
    
    # è½¬æ¢æ ‡ç­¾ä¸ºæ•°å€¼
    numeric_labels = [label_to_idx[label] for label in node_labels_list]
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    node_features = np.array(node_features_list, dtype=np.float32)
    node_labels = np.array(numeric_labels, dtype=np.int32)
    
    print("ğŸ”„ è¯»å–å›¾ç»“æ„...")
    
    # è¯»å–è¾¹ä¿¡æ¯
    adjacency_list = defaultdict(list)
    edge_count = 0
    
    with open(cites_file, 'r') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) != 2:
                continue
            
            cited_id, citing_id = parts
            
            # åªä¿ç•™åœ¨å½“å‰èŠ‚ç‚¹é›†åˆä¸­çš„è¾¹
            if cited_id in node_id_to_idx and citing_id in node_id_to_idx:
                cited_idx = node_id_to_idx[cited_id]
                citing_idx = node_id_to_idx[citing_id]
                
                # åˆ›å»ºæ— å‘å›¾
                adjacency_list[cited_idx].append(citing_idx)
                adjacency_list[citing_idx].append(cited_idx)
                edge_count += 1
    
    # å»é‡
    for node in adjacency_list:
        adjacency_list[node] = list(set(adjacency_list[node]))
    
    # ç‰¹å¾å½’ä¸€åŒ–
    feature_mean = np.mean(node_features, axis=0)
    feature_std = np.std(node_features, axis=0)
    feature_std[feature_std == 0] = 1  # é¿å…é™¤é›¶
    node_features = (node_features - feature_mean) / feature_std
    
    print("=" * 60)
    print("âœ… æ•°æ®åŠ è½½å®Œæˆ!")
    print(f"ğŸ“Š èŠ‚ç‚¹æ•°é‡: {len(node_features)}")
    print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {node_features.shape[1]}")
    print(f"ğŸ“Š ç±»åˆ«æ•°é‡: {len(label_to_idx)}")
    print(f"ğŸ“Š è¾¹æ•°é‡: {sum(len(neighbors) for neighbors in adjacency_list.values()) // 2}")
    print(f"ğŸ“Š ç±»åˆ«æ ‡ç­¾: {list(label_to_idx.keys())}")
    print("=" * 60)
    
    return node_features, node_labels, adjacency_list, label_to_idx, idx_to_label


def train_graphsage_complete(use_subset=True, subset_size=800):
    """å®Œæ•´çš„ GraphSAGE è®­ç»ƒæµç¨‹"""
    
    print("ğŸš€ GraphSAGE å®Œæ•´è®­ç»ƒæµç¨‹")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®
    node_features, node_labels, adjacency_list, label_to_idx, idx_to_label = load_citeseer_data(
        use_subset=use_subset, subset_size=subset_size
    )
    
    # åˆ†å‰²æ•°æ®
    num_nodes = len(node_features)
    indices = np.random.permutation(num_nodes)
    
    train_end = int(0.6 * num_nodes)
    val_end = int(0.8 * num_nodes)
    
    train_indices = indices[:train_end]
    val_indices = indices[train_end:val_end]
    test_indices = indices[val_end:]
    
    print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"   è®­ç»ƒé›†: {len(train_indices)} ä¸ªèŠ‚ç‚¹")
    print(f"   éªŒè¯é›†: {len(val_indices)} ä¸ªèŠ‚ç‚¹")
    print(f"   æµ‹è¯•é›†: {len(test_indices)} ä¸ªèŠ‚ç‚¹")
    
    # åˆå§‹åŒ–æ¨¡å‹
    input_dim = node_features.shape[1]
    hidden_dim = 64
    output_dim = len(label_to_idx)
    
    model = GraphSAGEComplete(
        input_dim=input_dim,
        hidden_dim=hidden_dim,
        output_dim=output_dim,
        num_layers=2,
        aggregator='mean'
    )
    
    print(f"\nğŸ§  æ¨¡å‹å‚æ•°:")
    print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
    print(f"   éšè—ç»´åº¦: {hidden_dim}")
    print(f"   è¾“å‡ºç»´åº¦: {output_dim}")
    print(f"   å±‚æ•°: 2")
    print(f"   èšåˆå‡½æ•°: mean")
    
    # è®­ç»ƒå‚æ•°
    epochs = 100
    learning_rate = 0.01
    batch_size = 32
    
    print(f"\nâš™ï¸ è®­ç»ƒå‚æ•°:")
    print(f"   è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # è®­ç»ƒå¾ªç¯
    train_losses = []
    train_accuracies = []
    val_accuracies = []
    
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print("=" * 70)
    
    start_time = time.time()
    
    for epoch in range(epochs):
        # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
        np.random.shuffle(train_indices)
        
        epoch_losses = []
        epoch_preds = []
        epoch_labels = []
        
        # æ‰¹æ¬¡è®­ç»ƒ
        for i in range(0, len(train_indices), batch_size):
            batch_indices = train_indices[i:i+batch_size]
            
            # å‰å‘ä¼ æ’­
            predictions = model.forward(node_features, adjacency_list, batch_indices)
            batch_labels = node_labels[batch_indices]
            
            # è®¡ç®—æŸå¤±
            loss = model.compute_loss(predictions, batch_labels)
            epoch_losses.append(loss)
            
            # è®¡ç®—é¢„æµ‹
            pred_labels = np.argmax(model._softmax(predictions), axis=1)
            epoch_preds.extend(pred_labels)
            epoch_labels.extend(batch_labels)
            
            # åå‘ä¼ æ’­
            model.backward(predictions, batch_labels, learning_rate)
        
        # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
        train_acc = np.mean(np.array(epoch_preds) == np.array(epoch_labels))
        
        # éªŒè¯é›†è¯„ä¼°
        val_predictions = model.forward(node_features, adjacency_list, val_indices)
        val_pred_labels = np.argmax(model._softmax(val_predictions), axis=1)
        val_acc = np.mean(val_pred_labels == node_labels[val_indices])
        
        # è®°å½•æŒ‡æ ‡
        avg_loss = np.mean(epoch_losses)
        train_losses.append(avg_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 10 == 0 or epoch == 0:
            elapsed = time.time() - start_time
            print(f"Epoch {epoch+1:3d}/{epochs}: Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f} (ç”¨æ—¶: {elapsed:.1f}s)")
    
    # æµ‹è¯•é›†è¯„ä¼°
    print("\n" + "=" * 50)
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯•...")
    
    test_predictions = model.forward(node_features, adjacency_list, test_indices)
    test_pred_labels = np.argmax(model._softmax(test_predictions), axis=1)
    test_acc = np.mean(test_pred_labels == node_labels[test_indices])
    
    print(f"âœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")
    
    # å„ç±»åˆ«å‡†ç¡®ç‡
    print("\nğŸ“Š å„ç±»åˆ«æµ‹è¯•å‡†ç¡®ç‡:")
    for class_idx, class_name in idx_to_label.items():
        class_mask = node_labels[test_indices] == class_idx
        if np.sum(class_mask) > 0:
            class_acc = np.mean(test_pred_labels[class_mask] == node_labels[test_indices][class_mask])
            print(f"   {class_name}: {class_acc:.4f} ({np.sum(class_mask)} ä¸ªæ ·æœ¬)")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    try:
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(train_losses)
        plt.title('è®­ç»ƒæŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)
        
        plt.subplot(1, 2, 2)
        plt.plot(train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡')
        plt.plot(val_accuracies, label='éªŒè¯å‡†ç¡®ç‡')
        plt.title('å‡†ç¡®ç‡æ›²çº¿')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('graphsage_real_data_training.png', dpi=150, bbox_inches='tight')
        print(f"\nğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º 'graphsage_real_data_training.png'")
        
    except Exception as e:
        print(f"ç»˜å›¾æ—¶å‡ºç°é”™è¯¯: {e}")
    
    total_time = time.time() - start_time
    print("=" * 70)
    print(f"ğŸ‰ GraphSAGE è®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
    print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    print("=" * 70)
    
    return model, test_acc


if __name__ == "__main__":
    print("ğŸ¯ GraphSAGE åœ¨çœŸå® Citeseer æ•°æ®é›†ä¸Šçš„å®Œæ•´å®ç°")
    print("ğŸ’¡ è¿™ä¸ªç‰ˆæœ¬åŒ…å«å®Œæ•´çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œæ¢¯åº¦ä¸‹é™")
    print()
    
    print("ğŸ¯ é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
    print("1. å®Œæ•´æ•°æ®é›†è®­ç»ƒ (3312 ä¸ªèŠ‚ç‚¹ï¼Œæ—¶é—´è¾ƒé•¿)")
    print("2. å°å­é›†è®­ç»ƒ (800 ä¸ªèŠ‚ç‚¹ï¼Œæ¨è)")
    print("3. ä¸­ç­‰å­é›†è®­ç»ƒ (1500 ä¸ªèŠ‚ç‚¹)")
    
    choice = input("è¯·é€‰æ‹© (1/2/3ï¼Œé»˜è®¤2): ").strip()
    
    if choice == "1":
        print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®é›†è®­ç»ƒ...")
        model, acc = train_graphsage_complete(use_subset=False)
    elif choice == "3":
        print("ğŸš€ å¼€å§‹ä¸­ç­‰å­é›†è®­ç»ƒ...")
        model, acc = train_graphsage_complete(use_subset=True, subset_size=1500)
    else:
        print("ğŸš€ å¼€å§‹å°å­é›†è®­ç»ƒ...")
        model, acc = train_graphsage_complete(use_subset=True, subset_size=800)
    
    print(f"\nğŸ† æœ€ç»ˆç»“æœ: æµ‹è¯•å‡†ç¡®ç‡ {acc:.4f}")
    print("âœ… è¿™æ¬¡çœŸçš„ä½¿ç”¨äº†çœŸå®çš„ Citeseer æ•°æ®é›†ï¼")