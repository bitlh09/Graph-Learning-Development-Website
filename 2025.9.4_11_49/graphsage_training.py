import numpy as np
import os
from collections import defaultdict
import time

print("GraphSAGEå®Œæ•´è®­ç»ƒå®ç°")
print("=" * 50)

class GraphSAGETrainer:
    """GraphSAGEæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01):
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.learning_rate = learning_rate
        
        # æƒé‡åˆå§‹åŒ– (Xavieråˆå§‹åŒ–)
        self.W1 = np.random.randn(input_dim * 2, hidden_dim) * np.sqrt(2.0 / (input_dim * 2))
        self.b1 = np.zeros(hidden_dim)
        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)
        self.b2 = np.zeros(output_dim)
        
        # ç”¨äºå­˜å‚¨å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ
        self.cache = {}
        
    def relu(self, x):
        return np.maximum(0, x)
    
    def relu_backward(self, dout, x):
        """ReLUçš„åå‘ä¼ æ’­"""
        dx = dout.copy()
        dx[x <= 0] = 0
        return dx
    
    def softmax(self, x):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def cross_entropy_loss(self, y_pred, y_true):
        """äº¤å‰ç†µæŸå¤±"""
        m = y_pred.shape[0]
        y_pred_clipped = np.clip(y_pred, 1e-12, 1 - 1e-12)
        
        # åˆ›å»ºone-hotç¼–ç 
        y_true_onehot = np.zeros_like(y_pred)
        y_true_onehot[np.arange(m), y_true] = 1
        
        # è®¡ç®—æŸå¤±
        loss = -np.sum(y_true_onehot * np.log(y_pred_clipped)) / m
        return loss
    
    def aggregate_neighbors(self, features, adj_list, nodes):
        """èšåˆé‚»å±…ç‰¹å¾"""
        agg_features = []
        for node in nodes:
            neighbors = adj_list.get(node, [])
            if len(neighbors) == 0:
                agg_features.append(np.zeros(features.shape[1]))
            else:
                valid_neighbors = [n for n in neighbors if n < len(features)]
                if len(valid_neighbors) == 0:
                    agg_features.append(np.zeros(features.shape[1]))
                else:
                    neighbor_feat = features[valid_neighbors]
                    agg_features.append(np.mean(neighbor_feat, axis=0))
        return np.array(agg_features)\n    \n    def forward(self, features, adj_list, nodes):\n        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n        batch_size = len(nodes)\n        \n        # èšåˆé‚»å±…ç‰¹å¾\n        neighbor_feat = self.aggregate_neighbors(features, adj_list, nodes)\n        self_feat = features[nodes]\n        \n        # è¿æ¥è‡ªèº«å’Œé‚»å±…ç‰¹å¾\n        combined = np.concatenate([self_feat, neighbor_feat], axis=1)\n        \n        # ç¬¬ä¸€å±‚\n        z1 = np.dot(combined, self.W1) + self.b1\n        h1 = self.relu(z1)\n        \n        # è¾“å‡ºå±‚\n        z2 = np.dot(h1, self.W2) + self.b2\n        y_pred = self.softmax(z2)\n        \n        # ä¿å­˜ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­\n        self.cache = {\n            'combined': combined,\n            'z1': z1,\n            'h1': h1,\n            'z2': z2,\n            'y_pred': y_pred\n        }\n        \n        return y_pred\n    \n    def backward(self, y_true):\n        \"\"\"åå‘ä¼ æ’­\"\"\"\n        # è·å–ç¼“å­˜çš„å€¼\n        combined = self.cache['combined']\n        z1 = self.cache['z1']\n        h1 = self.cache['h1']\n        z2 = self.cache['z2']\n        y_pred = self.cache['y_pred']\n        \n        batch_size = y_pred.shape[0]\n        \n        # åˆ›å»ºone-hotç¼–ç çš„çœŸå®æ ‡ç­¾\n        y_true_onehot = np.zeros_like(y_pred)\n        y_true_onehot[np.arange(batch_size), y_true] = 1\n        \n        # è¾“å‡ºå±‚çš„æ¢¯åº¦\n        dz2 = (y_pred - y_true_onehot) / batch_size\n        \n        # W2å’Œb2çš„æ¢¯åº¦\n        dW2 = np.dot(h1.T, dz2)\n        db2 = np.sum(dz2, axis=0)\n        \n        # éšè—å±‚çš„æ¢¯åº¦\n        dh1 = np.dot(dz2, self.W2.T)\n        dz1 = self.relu_backward(dh1, z1)\n        \n        # W1å’Œb1çš„æ¢¯åº¦\n        dW1 = np.dot(combined.T, dz1)\n        db1 = np.sum(dz1, axis=0)\n        \n        # æ›´æ–°æƒé‡\n        self.W2 -= self.learning_rate * dW2\n        self.b2 -= self.learning_rate * db2\n        self.W1 -= self.learning_rate * dW1\n        self.b1 -= self.learning_rate * db1\n    \n    def predict(self, features, adj_list, nodes):\n        \"\"\"é¢„æµ‹\"\"\"\n        y_pred = self.forward(features, adj_list, nodes)\n        return np.argmax(y_pred, axis=1)\n    \n    def train_step(self, features, adj_list, nodes, labels):\n        \"\"\"å•æ­¥è®­ç»ƒ\"\"\"\n        # å‰å‘ä¼ æ’­\n        y_pred = self.forward(features, adj_list, nodes)\n        \n        # è®¡ç®—æŸå¤±\n        loss = self.cross_entropy_loss(y_pred, labels)\n        \n        # åå‘ä¼ æ’­\n        self.backward(labels)\n        \n        # è®¡ç®—å‡†ç¡®ç‡\n        predictions = np.argmax(y_pred, axis=1)\n        accuracy = np.mean(predictions == labels)\n        \n        return loss, accuracy


def load_citeseer_data():\n    \"\"\"åŠ è½½citeseeræ•°æ®\"\"\"\n    print(\"åŠ è½½Citeseeræ•°æ®é›†...\")\n    \n    content_file = \"data/citeseer/citeseer.content\"\n    cites_file = \"data/citeseer/citeseer.cites\"\n    \n    # è¯»å–èŠ‚ç‚¹ç‰¹å¾å’Œæ ‡ç­¾\n    node_id_to_idx = {}\n    features_list = []\n    labels_list = []\n    \n    with open(content_file, 'r') as f:\n        for idx, line in enumerate(f):\n            parts = line.strip().split('\\t')\n            node_id = parts[0]\n            features = [float(x) for x in parts[1:-1]]\n            label = parts[-1]\n            \n            node_id_to_idx[node_id] = idx\n            features_list.append(features)\n            labels_list.append(label)\n    \n    # åˆ›å»ºæ ‡ç­¾æ˜ å°„\n    unique_labels = list(set(labels_list))\n    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n    numeric_labels = [label_to_idx[label] for label in labels_list]\n    \n    # è½¬æ¢ä¸ºnumpyæ•°ç»„\n    features = np.array(features_list, dtype=np.float32)\n    labels = np.array(numeric_labels, dtype=np.int32)\n    \n    # ç‰¹å¾å½’ä¸€åŒ–\n    features = (features - np.mean(features, axis=0)) / (np.std(features, axis=0) + 1e-8)\n    \n    # è¯»å–è¾¹ä¿¡æ¯\n    adj_list = defaultdict(list)\n    edge_count = 0\n    \n    with open(cites_file, 'r') as f:\n        for line in f:\n            parts = line.strip().split('\\t')\n            if len(parts) == 2:\n                cited_id, citing_id = parts\n                if cited_id in node_id_to_idx and citing_id in node_id_to_idx:\n                    cited_idx = node_id_to_idx[cited_id]\n                    citing_idx = node_id_to_idx[citing_id]\n                    adj_list[cited_idx].append(citing_idx)\n                    adj_list[citing_idx].append(cited_idx)\n                    edge_count += 1\n    \n    # å»é‡\n    for node in adj_list:\n        adj_list[node] = list(set(adj_list[node]))\n    \n    print(f\"æ•°æ®åŠ è½½å®Œæˆ:\")\n    print(f\"  èŠ‚ç‚¹æ•°: {len(features)}\")\n    print(f\"  ç‰¹å¾ç»´åº¦: {features.shape[1]}\")\n    print(f\"  ç±»åˆ«æ•°: {len(unique_labels)}\")\n    print(f\"  è¾¹æ•°: {edge_count}\")\n    print(f\"  ç±»åˆ«: {unique_labels}\")\n    \n    return features, labels, adj_list, label_to_idx, unique_labels


def main():\n    \"\"\"ä¸»è®­ç»ƒå‡½æ•°\"\"\"\n    # è®¾ç½®éšæœºç§å­\n    np.random.seed(42)\n    \n    # åŠ è½½æ•°æ®\n    features, labels, adj_list, label_to_idx, unique_labels = load_citeseer_data()\n    \n    # æ•°æ®åˆ†å‰²\n    num_nodes = len(features)\n    indices = np.random.permutation(num_nodes)\n    \n    train_size = int(0.6 * num_nodes)\n    val_size = int(0.2 * num_nodes)\n    \n    train_indices = indices[:train_size]\n    val_indices = indices[train_size:train_size + val_size]\n    test_indices = indices[train_size + val_size:]\n    \n    print(f\"\\næ•°æ®åˆ†å‰²:\")\n    print(f\"  è®­ç»ƒé›†: {len(train_indices)}\")\n    print(f\"  éªŒè¯é›†: {len(val_indices)}\")\n    print(f\"  æµ‹è¯•é›†: {len(test_indices)}\")\n    \n    # åˆå§‹åŒ–æ¨¡å‹\n    input_dim = features.shape[1]\n    hidden_dim = 128\n    output_dim = len(label_to_idx)\n    learning_rate = 0.01\n    \n    model = GraphSAGETrainer(input_dim, hidden_dim, output_dim, learning_rate)\n    \n    print(f\"\\næ¨¡å‹å‚æ•°:\")\n    print(f\"  è¾“å…¥ç»´åº¦: {input_dim}\")\n    print(f\"  éšè—ç»´åº¦: {hidden_dim}\")\n    print(f\"  è¾“å‡ºç»´åº¦: {output_dim}\")\n    print(f\"  å­¦ä¹ ç‡: {learning_rate}\")\n    \n    # è®­ç»ƒå‚æ•°\n    epochs = 50\n    batch_size = 32\n    \n    print(f\"\\nå¼€å§‹è®­ç»ƒ...\")\n    print(f\"  è½®æ•°: {epochs}\")\n    print(f\"  æ‰¹æ¬¡å¤§å°: {batch_size}\")\n    print(\"-\" * 50)\n    \n    # è®­ç»ƒè®°å½•\n    train_losses = []\n    train_accuracies = []\n    val_accuracies = []\n    \n    start_time = time.time()\n    \n    for epoch in range(epochs):\n        # æ‰“ä¹±è®­ç»ƒæ•°æ®\n        np.random.shuffle(train_indices)\n        \n        epoch_losses = []\n        epoch_accuracies = []\n        \n        # æ‰¹æ¬¡è®­ç»ƒ\n        for i in range(0, len(train_indices), batch_size):\n            batch_indices = train_indices[i:i+batch_size]\n            batch_labels = labels[batch_indices]\n            \n            # è®­ç»ƒæ­¥éª¤\n            loss, accuracy = model.train_step(features, adj_list, batch_indices, batch_labels)\n            \n            epoch_losses.append(loss)\n            epoch_accuracies.append(accuracy)\n        \n        # è®¡ç®—éªŒè¯å‡†ç¡®ç‡\n        val_predictions = model.predict(features, adj_list, val_indices)\n        val_accuracy = np.mean(val_predictions == labels[val_indices])\n        \n        # è®°å½•æŒ‡æ ‡\n        avg_loss = np.mean(epoch_losses)\n        avg_train_accuracy = np.mean(epoch_accuracies)\n        \n        train_losses.append(avg_loss)\n        train_accuracies.append(avg_train_accuracy)\n        val_accuracies.append(val_accuracy)\n        \n        # æ‰“å°è¿›åº¦\n        if (epoch + 1) % 5 == 0 or epoch == 0:\n            elapsed_time = time.time() - start_time\n            print(f\"Epoch {epoch+1:2d}/{epochs}: Loss: {avg_loss:.4f}, \"\n                  f\"Train Acc: {avg_train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, \"\n                  f\"Time: {elapsed_time:.1f}s\")\n    \n    print(\"-\" * 50)\n    print(\"è®­ç»ƒå®Œæˆï¼\")\n    \n    # æµ‹è¯•é›†è¯„ä¼°\n    print(\"\\næœ€ç»ˆè¯„ä¼°:\")\n    test_predictions = model.predict(features, adj_list, test_indices)\n    test_accuracy = np.mean(test_predictions == labels[test_indices])\n    \n    print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}\")\n    \n    # å„ç±»åˆ«å‡†ç¡®ç‡\n    print(\"\\nå„ç±»åˆ«æµ‹è¯•å‡†ç¡®ç‡:\")\n    for class_idx, class_name in enumerate(unique_labels):\n        class_mask = labels[test_indices] == class_idx\n        if np.sum(class_mask) > 0:\n            class_predictions = test_predictions[class_mask]\n            class_labels = labels[test_indices][class_mask]\n            class_accuracy = np.mean(class_predictions == class_labels)\n            print(f\"  {class_name}: {class_accuracy:.4f} ({np.sum(class_mask)} samples)\")\n    \n    # æ˜¾ç¤ºè®­ç»ƒæ›²çº¿æ•°æ®\n    print(\"\\nè®­ç»ƒæ›²çº¿æ•°æ®:\")\n    print(f\"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {train_accuracies[-1]:.4f}\")\n    print(f\"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_accuracies[-1]:.4f}\")\n    print(f\"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(val_accuracies):.4f} (Epoch {np.argmax(val_accuracies)+1})\")\n    \n    total_time = time.time() - start_time\n    print(f\"æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’\")\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"GraphSAGEè®­ç»ƒå®Œæˆï¼\")\n    print(\"å®ç°äº†å®Œæ•´çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œæ¢¯åº¦ä¸‹é™\")\n    print(\"=\" * 50)\n    \n    return model, test_accuracy, val_accuracies\n\n\nif __name__ == \"__main__\":\n    model, test_acc, val_accs = main()\n    print(f\"\\nğŸ‰ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}\")