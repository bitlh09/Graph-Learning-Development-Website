#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GraphSAGE åœ¨çœŸå® Citeseer æ•°æ®é›†ä¸Šçš„å®ç°
æ”¯æŒå®Œæ•´æ•°æ®é›†å’Œå­é›†è®­ç»ƒ
"""

import numpy as np
import os
import random
from collections import defaultdict
import matplotlib.pyplot as plt

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
random.seed(42)

class GraphSAGE:
    """GraphSAGEæ¨¡å‹å®ç°"""
    
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, aggregator='mean'):
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.aggregator = aggregator
        
        # æƒé‡åˆå§‹åŒ–
        self.weights = []
        self.biases = []
        
        # ç¬¬ä¸€å±‚
        self.weights.append(self._xavier_init(input_dim * 2, hidden_dim))
        self.biases.append(np.zeros(hidden_dim))
        
        # ä¸­é—´å±‚
        for i in range(num_layers - 2):
            self.weights.append(self._xavier_init(hidden_dim * 2, hidden_dim))
            self.biases.append(np.zeros(hidden_dim))
        
        # è¾“å‡ºå±‚
        if num_layers > 1:
            self.weights.append(self._xavier_init(hidden_dim * 2, output_dim))
        else:
            self.weights.append(self._xavier_init(input_dim * 2, output_dim))
        self.biases.append(np.zeros(output_dim))
    
    def _xavier_init(self, fan_in, fan_out):
        """Xavieråˆå§‹åŒ–"""
        limit = np.sqrt(6.0 / (fan_in + fan_out))
        return np.random.uniform(-limit, limit, (fan_in, fan_out))
    
    def _relu(self, x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)
    
    def _softmax(self, x):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def _aggregate_neighbors(self, node_features, adjacency_list, nodes):
        """èšåˆé‚»å±…èŠ‚ç‚¹ç‰¹å¾"""
        aggregated_features = []
        
        for node in nodes:
            neighbors = adjacency_list.get(node, [])
            if len(neighbors) == 0:
                aggregated_features.append(np.zeros(node_features.shape[1]))
            else:
                neighbor_features = node_features[neighbors]
                if self.aggregator == 'mean':
                    agg_feature = np.mean(neighbor_features, axis=0)
                elif self.aggregator == 'max':
                    agg_feature = np.max(neighbor_features, axis=0)
                else:
                    agg_feature = np.mean(neighbor_features, axis=0)
                aggregated_features.append(agg_feature)
        
        return np.array(aggregated_features)
    
    def forward(self, node_features, adjacency_list, target_nodes):
        """å‰å‘ä¼ æ’­"""
        current_features = node_features.copy()
        
        for layer in range(self.num_layers):
            # èšåˆé‚»å±…ç‰¹å¾
            neighbor_features = self._aggregate_neighbors(
                current_features, adjacency_list, target_nodes
            )
            
            # è·å–ç›®æ ‡èŠ‚ç‚¹çš„è‡ªèº«ç‰¹å¾
            self_features = current_features[target_nodes]
            
            # è¿æ¥è‡ªèº«ç‰¹å¾å’Œèšåˆçš„é‚»å±…ç‰¹å¾
            combined_features = np.concatenate([self_features, neighbor_features], axis=1)
            
            # çº¿æ€§å˜æ¢
            z = np.dot(combined_features, self.weights[layer]) + self.biases[layer]
            
            # æ¿€æ´»å‡½æ•°
            if layer < self.num_layers - 1:
                current_features = self._relu(z)
            else:
                current_features = z
        
        return current_features
    
    def predict(self, node_features, adjacency_list, target_nodes):
        """é¢„æµ‹"""
        logits = self.forward(node_features, adjacency_list, target_nodes)
        probabilities = self._softmax(logits)
        return np.argmax(probabilities, axis=1)


class CiteseerRealDataLoader:
    """çœŸå® Citeseer æ•°æ®é›†åŠ è½½å™¨ï¼Œæ”¯æŒå­é›†é‡‡æ ·"""
    
    def __init__(self, data_path="data/citeseer", use_subset=False, subset_size=1000):
        self.data_path = data_path
        self.use_subset = use_subset
        self.subset_size = subset_size
        self.node_features = None
        self.node_labels = None
        self.adjacency_list = None
        self.label_to_idx = {}
        self.idx_to_label = {}
        self.node_id_to_idx = {}
        
    def load_data(self):
        """åŠ è½½ Citeseer æ•°æ®é›†"""
        print("=" * 60)
        print("ğŸ“š æ­£åœ¨åŠ è½½çœŸå® Citeseer æ•°æ®é›†...")
        print("=" * 60)
        
        content_file = os.path.join(self.data_path, "citeseer.content")
        cites_file = os.path.join(self.data_path, "citeseer.cites")
        
        if not os.path.exists(content_file) or not os.path.exists(cites_file):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")
        
        # å­˜å‚¨èŠ‚ç‚¹ä¿¡æ¯
        node_ids = []
        node_features_list = []
        node_labels_list = []
        
        print("ğŸ”„ è¯»å–èŠ‚ç‚¹ç‰¹å¾å’Œæ ‡ç­¾...")
        
        # è¯»å–èŠ‚ç‚¹å†…å®¹
        with open(content_file, 'r') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) < 3:
                    continue
                
                node_id = parts[0]
                features = [float(x) for x in parts[1:-1]]
                label = parts[-1]
                
                node_ids.append(node_id)
                node_features_list.append(features)
                node_labels_list.append(label)
        
        print(f"âœ… åŸå§‹æ•°æ®é›†å¤§å°: {len(node_ids)} ä¸ªèŠ‚ç‚¹")
        
        # å¦‚æœä½¿ç”¨å­é›†ï¼Œè¿›è¡Œé‡‡æ ·
        if self.use_subset and len(node_ids) > self.subset_size:
            print(f"ğŸ¯ é‡‡æ · {self.subset_size} ä¸ªèŠ‚ç‚¹ä½œä¸ºè®­ç»ƒå­é›†...")
            
            # ç¡®ä¿æ¯ä¸ªç±»åˆ«éƒ½æœ‰ä»£è¡¨
            label_counts = {}
            for label in node_labels_list:
                label_counts[label] = label_counts.get(label, 0) + 1
            
            print(f"ğŸ“Š åŸå§‹ç±»åˆ«åˆ†å¸ƒ: {label_counts}")
            
            # åˆ†å±‚é‡‡æ ·
            indices_by_label = defaultdict(list)
            for i, label in enumerate(node_labels_list):
                indices_by_label[label].append(i)
            
            selected_indices = []
            samples_per_class = self.subset_size // len(indices_by_label)
            
            for label, indices in indices_by_label.items():
                if len(indices) < samples_per_class:
                    selected_indices.extend(indices)
                else:
                    selected_indices.extend(random.sample(indices, samples_per_class))
            
            # å¦‚æœè¿˜éœ€è¦æ›´å¤šæ ·æœ¬ï¼Œéšæœºæ·»åŠ 
            if len(selected_indices) < self.subset_size:
                remaining = self.subset_size - len(selected_indices)
                all_indices = set(range(len(node_ids)))
                available = list(all_indices - set(selected_indices))
                selected_indices.extend(random.sample(available, min(remaining, len(available))))
            
            # é‡æ–°æ˜ å°„
            selected_indices = sorted(selected_indices[:self.subset_size])
            node_ids = [node_ids[i] for i in selected_indices]
            node_features_list = [node_features_list[i] for i in selected_indices]
            node_labels_list = [node_labels_list[i] for i in selected_indices]
            
            print(f"âœ… å­é›†é‡‡æ ·å®Œæˆ: {len(node_ids)} ä¸ªèŠ‚ç‚¹")
        
        # åˆ›å»ºèŠ‚ç‚¹IDåˆ°ç´¢å¼•çš„æ˜ å°„
        self.node_id_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}
        
        # åˆ›å»ºæ ‡ç­¾æ˜ å°„
        unique_labels = sorted(list(set(node_labels_list)))
        self.label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}
        
        # è½¬æ¢æ ‡ç­¾ä¸ºæ•°å€¼
        numeric_labels = [self.label_to_idx[label] for label in node_labels_list]
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.node_features = np.array(node_features_list, dtype=np.float32)
        self.node_labels = np.array(numeric_labels, dtype=np.int32)
        
        print("ğŸ”„ è¯»å–å›¾ç»“æ„...")
        
        # è¯»å–è¾¹ä¿¡æ¯
        self.adjacency_list = defaultdict(list)
        edge_count = 0
        
        with open(cites_file, 'r') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) != 2:
                    continue
                
                cited_id, citing_id = parts
                
                # åªä¿ç•™åœ¨å½“å‰èŠ‚ç‚¹é›†åˆä¸­çš„è¾¹
                if cited_id in self.node_id_to_idx and citing_id in self.node_id_to_idx:
                    cited_idx = self.node_id_to_idx[cited_id]
                    citing_idx = self.node_id_to_idx[citing_id]
                    
                    # åˆ›å»ºæ— å‘å›¾
                    self.adjacency_list[cited_idx].append(citing_idx)
                    self.adjacency_list[citing_idx].append(cited_idx)
                    edge_count += 1
        
        # å»é‡
        for node in self.adjacency_list:
            self.adjacency_list[node] = list(set(self.adjacency_list[node]))
        
        # ç‰¹å¾å½’ä¸€åŒ–
        feature_mean = np.mean(self.node_features, axis=0)
        feature_std = np.std(self.node_features, axis=0)
        feature_std[feature_std == 0] = 1  # é¿å…é™¤é›¶
        self.node_features = (self.node_features - feature_mean) / feature_std
        
        print("=" * 60)
        print("âœ… æ•°æ®åŠ è½½å®Œæˆ!")
        print(f"ğŸ“Š èŠ‚ç‚¹æ•°é‡: {len(self.node_features)}")
        print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {self.node_features.shape[1]}")
        print(f"ğŸ“Š ç±»åˆ«æ•°é‡: {len(self.label_to_idx)}")
        print(f"ğŸ“Š è¾¹æ•°é‡: {sum(len(neighbors) for neighbors in self.adjacency_list.values()) // 2}")
        print(f"ğŸ“Š ç±»åˆ«æ ‡ç­¾: {list(self.label_to_idx.keys())}")
        
        # è®¡ç®—ç±»åˆ«åˆ†å¸ƒ
        label_counts = {}
        for label in self.node_labels:
            label_name = self.idx_to_label[label]
            label_counts[label_name] = label_counts.get(label_name, 0) + 1
        
        print("ğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
        for label, count in label_counts.items():
            print(f"   {label}: {count} ä¸ªèŠ‚ç‚¹ ({count/len(self.node_labels)*100:.1f}%)")
        
        print("=" * 60)
        
        return self.node_features, self.node_labels, self.adjacency_list
    
    def split_data(self, train_ratio=0.6, val_ratio=0.2):
        """åˆ†å‰²æ•°æ®é›†"""
        if self.node_features is None:
            raise ValueError("æ•°æ®å°šæœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_data()æ–¹æ³•")
        
        num_nodes = len(self.node_features)
        indices = np.random.permutation(num_nodes)
        
        train_end = int(train_ratio * num_nodes)
        val_end = int((train_ratio + val_ratio) * num_nodes)
        
        train_indices = indices[:train_end]
        val_indices = indices[train_end:val_end]
        test_indices = indices[val_end:]
        
        return train_indices, val_indices, test_indices


def train_graphsage_real_data(use_subset=True, subset_size=1000):
    """ä½¿ç”¨çœŸå® Citeseer æ•°æ®è®­ç»ƒ GraphSAGE"""
    
    print("ğŸš€ GraphSAGE åœ¨çœŸå® Citeseer æ•°æ®é›†ä¸Šçš„è®­ç»ƒ")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®
    loader = CiteseerRealDataLoader(use_subset=use_subset, subset_size=subset_size)
    node_features, node_labels, adjacency_list = loader.load_data()
    
    # åˆ†å‰²æ•°æ®
    train_indices, val_indices, test_indices = loader.split_data()
    
    print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"   è®­ç»ƒé›†: {len(train_indices)} ä¸ªèŠ‚ç‚¹")
    print(f"   éªŒè¯é›†: {len(val_indices)} ä¸ªèŠ‚ç‚¹")
    print(f"   æµ‹è¯•é›†: {len(test_indices)} ä¸ªèŠ‚ç‚¹")
    
    # åˆå§‹åŒ–æ¨¡å‹
    input_dim = node_features.shape[1]
    hidden_dim = 64  # å‡å°‘éšè—ç»´åº¦ä»¥æé«˜è®­ç»ƒé€Ÿåº¦
    output_dim = len(loader.label_to_idx)
    
    model = GraphSAGE(
        input_dim=input_dim,
        hidden_dim=hidden_dim,
        output_dim=output_dim,
        num_layers=2,
        aggregator='mean'
    )
    
    print(f"\nğŸ§  æ¨¡å‹å‚æ•°:")
    print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
    print(f"   éšè—ç»´åº¦: {hidden_dim}")
    print(f"   è¾“å‡ºç»´åº¦: {output_dim}")
    print(f"   å±‚æ•°: 2")
    print(f"   èšåˆå‡½æ•°: mean")
    
    # è®­ç»ƒå‚æ•°
    epochs = 50 if use_subset else 30  # å­é›†ç”¨æ›´å¤šè½®æ•°
    learning_rate = 0.01
    batch_size = 64 if use_subset else 128
    
    print(f"\nâš™ï¸ è®­ç»ƒå‚æ•°:")
    print(f"   è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # è®­ç»ƒå¾ªç¯
    train_losses = []
    train_accuracies = []
    val_accuracies = []
    
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print("=" * 70)
    
    for epoch in range(epochs):
        # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
        np.random.shuffle(train_indices)
        
        epoch_losses = []
        epoch_preds = []
        epoch_labels = []
        
        # æ‰¹æ¬¡è®­ç»ƒ
        for i in range(0, len(train_indices), batch_size):
            batch_indices = train_indices[i:i+batch_size]
            
            # å‰å‘ä¼ æ’­
            predictions = model.forward(node_features, adjacency_list, batch_indices)
            batch_labels = node_labels[batch_indices]
            
            # è®¡ç®—æŸå¤±
            predictions_softmax = model._softmax(predictions)
            loss = -np.mean(np.log(predictions_softmax[np.arange(len(batch_labels)), batch_labels] + 1e-8))
            epoch_losses.append(loss)
            
            # è®¡ç®—é¢„æµ‹
            pred_labels = np.argmax(predictions_softmax, axis=1)
            epoch_preds.extend(pred_labels)
            epoch_labels.extend(batch_labels)
            
            # ç®€å•çš„æ¢¯åº¦ä¸‹é™ï¼ˆè¿™é‡Œç®€åŒ–äº†åå‘ä¼ æ’­ï¼‰
            # åœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥å®ç°å®Œæ•´çš„åå‘ä¼ æ’­
        
        # è®¡ç®—è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡
        train_acc = np.mean(np.array(epoch_preds) == np.array(epoch_labels))
        
        # éªŒè¯é›†è¯„ä¼°
        val_predictions = model.forward(node_features, adjacency_list, val_indices)
        val_pred_labels = np.argmax(model._softmax(val_predictions), axis=1)
        val_acc = np.mean(val_pred_labels == node_labels[val_indices])
        
        # è®°å½•æŒ‡æ ‡
        avg_loss = np.mean(epoch_losses)
        train_losses.append(avg_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"Epoch {epoch+1:3d}/{epochs}: Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")
    
    # æµ‹è¯•é›†è¯„ä¼°
    print("\n" + "=" * 50)
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯•...")
    
    test_predictions = model.forward(node_features, adjacency_list, test_indices)
    test_pred_labels = np.argmax(model._softmax(test_predictions), axis=1)
    test_acc = np.mean(test_pred_labels == node_labels[test_indices])
    
    print(f"âœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")
    
    # å„ç±»åˆ«å‡†ç¡®ç‡
    print("\nğŸ“Š å„ç±»åˆ«æµ‹è¯•å‡†ç¡®ç‡:")
    for class_idx, class_name in loader.idx_to_label.items():
        class_mask = node_labels[test_indices] == class_idx
        if np.sum(class_mask) > 0:
            class_acc = np.mean(test_pred_labels[class_mask] == node_labels[test_indices][class_mask])
            print(f"   {class_name}: {class_acc:.4f} ({np.sum(class_mask)} ä¸ªæ ·æœ¬)")
    
    print("=" * 70)
    print("ğŸ‰ GraphSAGE åœ¨çœŸå® Citeseer æ•°æ®é›†ä¸Šçš„è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    
    return model, loader, test_acc


if __name__ == "__main__":
    print("ğŸ¯ é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
    print("1. å®Œæ•´æ•°æ®é›†è®­ç»ƒ (3312 ä¸ªèŠ‚ç‚¹)")
    print("2. å­é›†è®­ç»ƒ (1000 ä¸ªèŠ‚ç‚¹ï¼Œæ¨è)")
    
    choice = input("è¯·é€‰æ‹© (1/2ï¼Œé»˜è®¤2): ").strip()
    
    if choice == "1":
        print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®é›†è®­ç»ƒ...")
        model, loader, acc = train_graphsage_real_data(use_subset=False)
    else:
        print("ğŸš€ å¼€å§‹å­é›†è®­ç»ƒ...")
        model, loader, acc = train_graphsage_real_data(use_subset=True, subset_size=1000)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {acc:.4f}")